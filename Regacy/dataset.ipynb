{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from conv import Convolutional_code\n",
    "from mod import Modulation\n",
    "from AWGN import AWGN\n",
    "import os\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 X 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 셋 폴더 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|██████████| 41/41 [00:36<00:00,  1.13it/s]\n",
      "Loading Files: 100%|██████████| 41/41 [00:40<00:00,  1.02it/s]\n",
      "Loading Files: 100%|██████████| 41/41 [00:40<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# SNR 범위 설정\n",
    "SNR = range(-20, 21, 1)\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "# 데이터 셋 폴더 생성 위치\n",
    "\n",
    "current_dir = Path('dataset.ipynb').parent\n",
    "\n",
    "dest = current_dir / 'DATASET_FOR_GAF_1_128'\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.mkdir(dest)\n",
    "    print(\"데이터 셋 폴더 생성\")\n",
    "\n",
    "# 데이터와 레이블을 저장할 리스트 초기화\n",
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "for CL in range(3):\n",
    "    # 채널 코딩에 사용할 콘볼루션 코드 선택 (1, 2, or 3)\n",
    "    convolution_code = CL + 1\n",
    "    modulation = 1\n",
    "    \n",
    "    encoder = Convolutional_code(convolution_code)\n",
    "    mod = Modulation(modulation)\n",
    "    \n",
    "    for i in tqdm(SNR, desc=\"Loading Files\"):\n",
    "        for j in range(1, num_samples+1):\n",
    "            \n",
    "            all_data = []\n",
    " \n",
    "            if CL == 0:\n",
    "                SIZE = 62\n",
    "            elif CL == 1:\n",
    "                SIZE = 61\n",
    "            else:\n",
    "                SIZE = 60\n",
    "            random_bits = np.random.randint(2, size=SIZE)\n",
    "            \n",
    "            # 채널코딩\n",
    "            encoded, _ = encoder.encoder(list(random_bits))\n",
    "\n",
    "            # 변조\n",
    "            moded = mod.mod(encoded)\n",
    "            \n",
    "            # 채널 효과 적용\n",
    "            noise_data = AWGN(int(i), 1/2, 1, moded)\n",
    "            \n",
    "            # 복조\n",
    "            data = mod.demod(noise_data, 1)\n",
    "            \n",
    "            # 넘파이 배열로 변경\n",
    "            data = np.array(data).reshape(1, 128)\n",
    "            all_data.append(data)\n",
    "  \n",
    "            # 데이터 리스트에 추가\n",
    "            data_list.append(all_data)\n",
    "            label_list.append((CL, i))  # CL과 SNR 값을 레이블로 사용\n",
    "\n",
    "# 데이터를 넘파이 배열로 변환\n",
    "data_array = np.array(data_list)\n",
    "label_array = np.array(label_list)\n",
    "\n",
    "# 데이터 셔플링\n",
    "np.random.seed(42)\n",
    "indices = np.arange(data_array.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data_array = data_array[indices]\n",
    "label_array = label_array[indices]\n",
    "\n",
    "# 기존 데이터 분할 (70% 훈련, 30% 테스트)\n",
    "# 훈련, 테스트, 검증 데이터 분할 (80% 훈련, 10% 테스트, 10% 검증)\n",
    "n_examples = data_array.shape[0]\n",
    "n_train = int(0.8 * n_examples)\n",
    "n_test = int(0.1 * n_examples)\n",
    "n_val = n_examples - n_train - n_test\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:n_train + n_test]\n",
    "val_idx = indices[n_train + n_test:]\n",
    "\n",
    "X_train = data_array[train_idx]\n",
    "X_test = data_array[test_idx]\n",
    "X_val = data_array[val_idx]\n",
    "\n",
    "Y_train = label_array[train_idx]\n",
    "Y_test = label_array[test_idx]\n",
    "Y_val = label_array[val_idx]\n",
    "\n",
    "# 원-핫 인코딩 함수\n",
    "def to_onehot(labels, num_classes):\n",
    "    onehot_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        onehot_labels[i, label] = 1\n",
    "    return onehot_labels\n",
    "\n",
    "# 레이블의 첫 번째 열(CL)을 원-핫 인코딩 (3개의 클래스)\n",
    "num_classes = len(set(label_array[:, 0]))  # 실제 클래스 수 계산\n",
    "Y_train_onehot = to_onehot(Y_train[:, 0], num_classes)\n",
    "Y_test_onehot = to_onehot(Y_test[:, 0], num_classes)\n",
    "Y_val_onehot = to_onehot(Y_val[:, 0], num_classes)\n",
    "\n",
    "# idx도 저장\n",
    "np.save(os.path.join(dest, \"train_idx.npy\"), train_idx)\n",
    "np.save(os.path.join(dest, \"test_idx.npy\"), test_idx)\n",
    "np.save(os.path.join(dest, \"val_idx.npy\"), val_idx)\n",
    "\n",
    "# npy 파일로 저장\n",
    "np.save(os.path.join(dest, \"lbl.npy\"), label_array)\n",
    "np.save(os.path.join(dest, \"x_train.npy\"), X_train)\n",
    "np.save(os.path.join(dest, \"x_test.npy\"), X_test)\n",
    "np.save(os.path.join(dest, \"x_val.npy\"), X_val)\n",
    "np.save(os.path.join(dest, \"y_train.npy\"), Y_train_onehot)\n",
    "np.save(os.path.join(dest, \"y_test.npy\"), Y_test_onehot)\n",
    "np.save(os.path.join(dest, \"y_val.npy\"), Y_val_onehot)\n",
    "\n",
    "print(\"데이터 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 0 1 1 0 0 1 0 1]\n",
      "[1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "[1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "random_bits = np.random.randint(2, size=(14))\n",
    "\n",
    "encoder = Convolutional_code(1)\n",
    "encoded, _ = encoder.encoder(list(random_bits))\n",
    "\n",
    "print(random_bits)\n",
    "print(list(random_bits))\n",
    "print(encoded)\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 X 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 생성 완료\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# SNR 범위 설정\n",
    "SNR = range(-5, 21, 1)\n",
    "num_samples = 1000\n",
    "\n",
    "# 데이터 셋 폴더 생성 위치\n",
    "current_dir = Path('dataset.ipynb').parent\n",
    "dest = current_dir / 'DATASET_16'\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.mkdir(dest)\n",
    "    print(\"데이터 셋 폴더 생성\")\n",
    "\n",
    "# 데이터와 레이블을 저장할 리스트 초기화\n",
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "def process_sample(CL, SNR_value):\n",
    "    encoder = Convolutional_code(CL + 1)\n",
    "    mod = Modulation(1)\n",
    "    \n",
    "    SIZE = 6 if CL == 0 else 5 if CL == 1 else 4\n",
    "\n",
    "    # 16*16 크기의 비트 스트림 생성\n",
    "    random_bits = np.random.randint(2, size=(16, SIZE))\n",
    "\n",
    "    all_data_list = []\n",
    "\n",
    "    for bits in random_bits:\n",
    "        # 채널 코딩\n",
    "        encoded, _ = encoder.encoder(list(bits))\n",
    "\n",
    "        # 변조\n",
    "        moded = mod.mod(encoded)\n",
    "\n",
    "        # 채널 효과 적용\n",
    "        noise_data = AWGN(int(SNR_value), 1/2, 1, moded)\n",
    "\n",
    "        # 복조\n",
    "        data = mod.demod(noise_data, 1)\n",
    "\n",
    "        all_data_list.append(data[:16])  # 필요한 길이만큼 자르기\n",
    "\n",
    "    # 128개의 1x128 배열을 128x128 배열로 변환\n",
    "    all_data = np.vstack(all_data_list)\n",
    "    return all_data, (CL, SNR_value)\n",
    "\n",
    "# 병렬 처리\n",
    "results = Parallel(n_jobs=-1)(delayed(process_sample)(CL, i) for CL in range(3) for i in SNR for j in range(1, num_samples + 1))\n",
    "\n",
    "# 결과 분리\n",
    "data_list, label_list = zip(*results)\n",
    "\n",
    "# 데이터를 넘파이 배열로 변환\n",
    "data_array = np.array(data_list)\n",
    "label_array = np.array(label_list)\n",
    "\n",
    "# 데이터 셔플링\n",
    "np.random.seed(42)\n",
    "indices = np.arange(data_array.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data_array = data_array[indices]\n",
    "label_array = label_array[indices]\n",
    "\n",
    "# 훈련, 테스트, 검증 데이터 분할 (80% 훈련, 10% 테스트, 10% 검증)\n",
    "n_examples = data_array.shape[0]\n",
    "n_train = int(0.8 * n_examples)\n",
    "n_test = int(0.1 * n_examples)\n",
    "n_val = n_examples - n_train - n_test\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:n_train + n_test]\n",
    "val_idx = indices[n_train + n_test:]\n",
    "\n",
    "X_train = data_array[train_idx]\n",
    "X_test = data_array[test_idx]\n",
    "X_val = data_array[val_idx]\n",
    "\n",
    "Y_train = label_array[train_idx]\n",
    "Y_test = label_array[test_idx]\n",
    "Y_val = label_array[val_idx]\n",
    "\n",
    "# 원-핫 인코딩 함수\n",
    "def to_onehot(labels, num_classes):\n",
    "    onehot_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        onehot_labels[i, label] = 1\n",
    "    return onehot_labels\n",
    "\n",
    "# 레이블의 첫 번째 열(CL)을 원-핫 인코딩 (3개의 클래스)\n",
    "num_classes = len(set(label_array[:, 0]))  # 실제 클래스 수 계산\n",
    "Y_train_onehot = to_onehot(Y_train[:, 0], num_classes)\n",
    "Y_test_onehot = to_onehot(Y_test[:, 0], num_classes)\n",
    "Y_val_onehot = to_onehot(Y_val[:, 0], num_classes)\n",
    "\n",
    "# npy 파일로 저장\n",
    "np.save(os.path.join(dest, \"x_train.npy\"), X_train)\n",
    "np.save(os.path.join(dest, \"x_test.npy\"), X_test)\n",
    "np.save(os.path.join(dest, \"x_val.npy\"), X_val)\n",
    "\n",
    "np.save(os.path.join(dest, \"y_train.npy\"), Y_train_onehot)\n",
    "np.save(os.path.join(dest, \"y_test.npy\"), Y_test_onehot)\n",
    "np.save(os.path.join(dest, \"y_val.npy\"), Y_val_onehot)\n",
    "\n",
    "np.save(os.path.join(dest, \"lbl.npy\"), label_array)\n",
    "\n",
    "# idx도 저장\n",
    "np.save(os.path.join(dest, \"train_idx.npy\"), train_idx)\n",
    "np.save(os.path.join(dest, \"test_idx.npy\"), test_idx)\n",
    "np.save(os.path.join(dest, \"val_idx.npy\"), val_idx)\n",
    "\n",
    "print(\"데이터 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32 X 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 생성 완료\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# SNR 범위 설정\n",
    "SNR = range(-5, 21, 1)\n",
    "num_samples = 1000\n",
    "\n",
    "# 데이터 셋 폴더 생성 위치\n",
    "current_dir = Path('dataset.ipynb').parent\n",
    "dest = current_dir / 'DATASET_32'\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.mkdir(dest)\n",
    "    print(\"데이터 셋 폴더 생성\")\n",
    "\n",
    "# 데이터와 레이블을 저장할 리스트 초기화\n",
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "def process_sample(CL, SNR_value):\n",
    "    encoder = Convolutional_code(CL + 1)\n",
    "    mod = Modulation(1)\n",
    "    \n",
    "    SIZE = 14 if CL == 0 else 13 if CL == 1 else 12\n",
    "\n",
    "    # 32*32 크기의 비트 스트림 생성\n",
    "    random_bits = np.random.randint(2, size=(32, SIZE))\n",
    "\n",
    "    all_data_list = []\n",
    "\n",
    "    for bits in random_bits:\n",
    "        # 채널 코딩\n",
    "        encoded, _ = encoder.encoder(list(bits))\n",
    "\n",
    "        # 변조\n",
    "        moded = mod.mod(encoded)\n",
    "\n",
    "        # 채널 효과 적용\n",
    "        noise_data = AWGN(int(SNR_value), 1/2, moded)\n",
    "\n",
    "        # 복조\n",
    "        data = mod.demod(noise_data, 1)\n",
    "\n",
    "        all_data_list.append(data[:32])  # 필요한 길이만큼 자르기\n",
    "\n",
    "    # 128개의 1x128 배열을 128x128 배열로 변환\n",
    "    all_data = np.vstack(all_data_list)\n",
    "    return all_data, (CL, SNR_value)\n",
    "\n",
    "# 병렬 처리\n",
    "results = Parallel(n_jobs=-1)(delayed(process_sample)(CL, i) for CL in range(3) for i in SNR for j in range(1, num_samples + 1))\n",
    "\n",
    "# 결과 분리\n",
    "data_list, label_list = zip(*results)\n",
    "\n",
    "# 데이터를 넘파이 배열로 변환\n",
    "data_array = np.array(data_list)\n",
    "label_array = np.array(label_list)\n",
    "\n",
    "# 데이터 셔플링\n",
    "np.random.seed(42)\n",
    "indices = np.arange(data_array.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data_array = data_array[indices]\n",
    "label_array = label_array[indices]\n",
    "\n",
    "# 훈련, 테스트, 검증 데이터 분할 (80% 훈련, 10% 테스트, 10% 검증)\n",
    "n_examples = data_array.shape[0]\n",
    "n_train = int(0.8 * n_examples)\n",
    "n_test = int(0.1 * n_examples)\n",
    "n_val = n_examples - n_train - n_test\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:n_train + n_test]\n",
    "val_idx = indices[n_train + n_test:]\n",
    "\n",
    "X_train = data_array[train_idx]\n",
    "X_test = data_array[test_idx]\n",
    "X_val = data_array[val_idx]\n",
    "\n",
    "Y_train = label_array[train_idx]\n",
    "Y_test = label_array[test_idx]\n",
    "Y_val = label_array[val_idx]\n",
    "\n",
    "# 원-핫 인코딩 함수\n",
    "def to_onehot(labels, num_classes):\n",
    "    onehot_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        onehot_labels[i, label] = 1\n",
    "    return onehot_labels\n",
    "\n",
    "# 레이블의 첫 번째 열(CL)을 원-핫 인코딩 (3개의 클래스)\n",
    "num_classes = len(set(label_array[:, 0]))  # 실제 클래스 수 계산\n",
    "Y_train_onehot = to_onehot(Y_train[:, 0], num_classes)\n",
    "Y_test_onehot = to_onehot(Y_test[:, 0], num_classes)\n",
    "Y_val_onehot = to_onehot(Y_val[:, 0], num_classes)\n",
    "\n",
    "# npy 파일로 저장\n",
    "np.save(os.path.join(dest, \"x_train.npy\"), X_train)\n",
    "np.save(os.path.join(dest, \"x_test.npy\"), X_test)\n",
    "np.save(os.path.join(dest, \"x_val.npy\"), X_val)\n",
    "\n",
    "np.save(os.path.join(dest, \"y_train.npy\"), Y_train_onehot)\n",
    "np.save(os.path.join(dest, \"y_test.npy\"), Y_test_onehot)\n",
    "np.save(os.path.join(dest, \"y_val.npy\"), Y_val_onehot)\n",
    "\n",
    "np.save(os.path.join(dest, \"lbl.npy\"), label_array)\n",
    "\n",
    "# idx도 저장\n",
    "np.save(os.path.join(dest, \"train_idx.npy\"), train_idx)\n",
    "np.save(os.path.join(dest, \"test_idx.npy\"), test_idx)\n",
    "np.save(os.path.join(dest, \"val_idx.npy\"), val_idx)\n",
    "\n",
    "print(\"데이터 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (23, 12) Golay code\n",
    "def golay(data_size):\n",
    "    m_len = 12\n",
    "\n",
    "    P = np.array([\n",
    "        [1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1],\n",
    "        [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0],\n",
    "        [0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1],\n",
    "        [1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "        [0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1],\n",
    "        [1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "        [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "    ], dtype='int')\n",
    "\n",
    "    G = np.concatenate((np.eye(m_len, dtype=\"int\"), P), axis=1)\n",
    "\n",
    "    msg = np.random.randint(0,2, (data_size, m_len))\n",
    "    codeword = np.dot(msg, G) %2\n",
    "\n",
    "    return codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming Code (8 4)\n",
    "def Hamming(data_size):\n",
    "    G = np.array([\n",
    "        [1, 1, 1, 0, 0 ,0, 0, 1],\n",
    "        [1, 0 ,0, 1, 1, 0, 0, 1],\n",
    "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0]\n",
    "    ], dtype='int')\n",
    "\n",
    "    msg = np.random.randint(0,2, (data_size, 4))\n",
    "    codeword = np.dot(msg, G) %2\n",
    "\n",
    "    return codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCH Code (15, 7)\n",
    "def BCH(data_size):\n",
    "    c_length = 15\n",
    "    m_length = 7\n",
    "    G_X_15_7 = [1,1,1,0,1,0,0,0,1]\n",
    "\n",
    "    BCH_15_7_G = np.zeros((7,15))\n",
    "\n",
    "    for i in range(7):\n",
    "        BCH_15_7_G[i,i:i+9] = G_X_15_7\n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(i+1,7):\n",
    "            if BCH_15_7_G[i, j] == 1:\n",
    "                BCH_15_7_G[i] = (BCH_15_7_G[i] + BCH_15_7_G[j])%2\n",
    "\n",
    "    G = BCH_15_7_G # G =BCH_7_4_G\n",
    "    msg = np.random.randint(0,2,(data_size,m_length))\n",
    "    codeword = np.dot(msg, G)%2\n",
    "\n",
    "    return codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Code (8, 4)\n",
    "def product(data_size):\n",
    "    G = np.array([\n",
    "        [1, 0, 1, 0, 0 ,0, 1, 0],\n",
    "        [0, 1 ,1, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 1, 0, 1, 1, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 1]\n",
    "    ], dtype='int')\n",
    "\n",
    "    msg = np.random.randint(0,2, (data_size, 4))\n",
    "    codeword = np.dot(msg, G) %2\n",
    "\n",
    "    return codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM Code\n",
    "def G_matrix(length, m, r):\n",
    "    G = np.ones(length)\n",
    "    for i in range(m):\n",
    "        v = np.zeros((int(length/(2**(i+1)))))\n",
    "        v = np.hstack((v, np.ones((int(length/(2**(i+1)))))))\n",
    "        while v.shape[0] < length :\n",
    "            v = np.hstack((v, np.zeros((int(length/(2**(i+1)))))))\n",
    "            v = np.hstack((v, np.ones((int(length/(2**(i+1)))))))\n",
    "        G = np.vstack((G,v))\n",
    "    if r == 1:\n",
    "        return G\n",
    "    elif r > 1 :\n",
    "        for i in range(1,m):\n",
    "            for j in range(i+1,m+1):\n",
    "                G = np.vstack((G,(G[i]*G[j])))\n",
    "        if r == 3:\n",
    "            G = np.vstack((G,(G[1]*G[2]*G[3])))\n",
    "            G = np.vstack((G,(G[1]*G[3]*G[4])))\n",
    "            G = np.vstack((G,(G[1]*G[2]*G[4])))\n",
    "            G = np.vstack((G,(G[2]*G[3]*G[4])))\n",
    "        return G\n",
    "    return G\n",
    "\n",
    "def rm(data_size):\n",
    "    m = 4\n",
    "    r = 2\n",
    "    length = 2**m\n",
    "\n",
    "    if r == 1:\n",
    "        masking_length=0\n",
    "        msg_length = m+r\n",
    "    elif r == 2:\n",
    "        masking_length=6\n",
    "        msg_length = 11\n",
    "    elif r == 3:\n",
    "        masking_length=10\n",
    "        msg_length = 15\n",
    "\n",
    "    G = G_matrix(length, m, r)\n",
    "    msg = np.random.randint(0,2,(data_size,msg_length))\n",
    "    codeword = np.dot(msg, G) %2\n",
    "\n",
    "    return codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n, k) Polar code\n",
    "\n",
    "def make_F(power):\n",
    "    F = np.array([[1,0],[1,1]])\n",
    "    for i in range(1,power):\n",
    "        first = np.concatenate((F,np.zeros((2**i, 2**i))), axis=1)\n",
    "        second = np.concatenate((F,F), axis=1)\n",
    "        F = np.concatenate((first, second), axis =0)\n",
    "\n",
    "    return F\n",
    "def Compute_z(z, k, i = 1):\n",
    "    for j in range(i):\n",
    "        z[(2*i,2*j)] = 2*z[(i,j)] - (z[(i,j)])**2\n",
    "        z[(2*i,2*j+1)] = (z[(i,j)])**2\n",
    "    if 2*i < 2**k:\n",
    "        z = Compute_z(z, k, 2*i)\n",
    "\n",
    "    return z\n",
    "def Frozen_bits(n, z, slice_index):\n",
    "    bit_index = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        bit_index[i] = z[(n, i)]\n",
    "\n",
    "    bit_index = np.argsort(bit_index)[::-1]\n",
    "\n",
    "    frozen_bit_index = bit_index[:slice_index]\n",
    "    message_bit_index = bit_index[slice_index:]\n",
    "    \n",
    "    return np.sort(frozen_bit_index), np.sort(message_bit_index)\n",
    "\n",
    "def polar(data_size):\n",
    "    # (n, k) Polar code\n",
    "    k = 4\n",
    "    n = 2*k\n",
    "    power = int(np.log2(n))\n",
    "\n",
    "    F = make_F(power)\n",
    "\n",
    "    z = {}\n",
    "    z[(1,0)] = 0.5\n",
    "    z = Compute_z(z, power)\n",
    "\n",
    "    frozen_bit_index, message_bit_index = Frozen_bits(n, z, int(n-k))\n",
    "\n",
    "    msg = np.random.randint(0,2,(data_size, k))\n",
    "    u = np.zeros((data_size, n))\n",
    "    u[:,message_bit_index] = msg\n",
    "\n",
    "    codeword = np.dot(u,F)%2\n",
    "\n",
    "    return codeword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
